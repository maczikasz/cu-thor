# Things to cover

## API

### High level Library API

The library API should provide the following methods to the clients:

* Initialize an executor instance with the following settings
  * Limits for resources
    

* Execute a command with the supplied arguments, that returns a pointer to a `Command` object that can be later used to manipulate the underlying process

### GRPC API definition

The GRPC API defines the following methods:

* ExecuteCommand: Starts a command with limited resources in its own namespace with the supplied arguments
* StopExecution : Stop a certain execution, issues a SIGTERM than after a configured amount of seconds issues a SIGKILL
* ListAllExecutions: List all the executions the connected client has rights to
* GetExecution: Gets metadata about a certain execution defined by an execution ID
* GetOutStream: Get a stream of standard output (stdout & stderr) of a certain execution

To read about the exact details of the API you can look at [the proto file](api.proto). The server will return UUIDs as ExecutionId that will be then used to later identify a certain execution. The reason for not using PID as an ExecutionId is because PIDs could be reused which could lead to unexpected behaviour from the application

### Streaming data

In order to be able to stream and also send the full output of the processes we need to store the output of this in memory for simplicty. See [Tradeoffs](#tradeoffs) section for information about why this solution should not be used in production  

## Security

### Cipher suite

In order to have the latest and strongest transport encryption TLS 1.3 should be utilized. The cipher suite TLS_AES_128_GCM_SHA256 is the mandatory cipher suite for TLS 1.3, and it provides sufficiently strong encryption there for it will be used by the server
The decision to ultimately use TLS 1.3 is due to the fact that the client and the server controlled by the same entity, therefore it's easy to enforce the usage of the latest version

### Authentication

### mTLS

For the mTLS the following certificates will be generated by a generate-certs shell script to enable easy testing of the application (see the Tradeoff section for more details):

* A root CA certificate to sign client and server CSR-s 
* A x509 certificate for the server signed by the root CA
* Two x509 certificates one for each of two clients, that could be used to connect to the applicaiton

The root CA will use a private key that's 4096 bits and the client and server certs will use private keys with 2048 bits

### Authorization

To keep things simple, the following statement should hold for every process started on the server:

`Only clients authenticated with the client certs having the same CN can get any information about, or interact with a certain process`

## Process execution

### Resource control

To control resource allocation linux control groups (cgroups v2) can be utilized

The following cgroups v2 controller parameters will be used to provide resource limitation:

* CPU: `cpu.max` to limit the available cput percentage
* Memory: `memory.max` and `memory.high` to set a hard and a soft limit for memory usage
* IO: `io.max` with the `rbps` and `wbps` parameters will be used to enforce Write and Read limit on all block devices 

The job executor library will create a separate cgroup for each process, this way it can limit the cpu percentage per process
The app will start the processes first then add their PIDs to the `cgroup.procs` file in the associated cgroup
See [Tradeoffs](#tradeoffs) section for information about how this could be improved

### Isolation

To implement isolation of namespaces for PID, Mount and Networks unix namespaces will be used

The app will implement a simple isolation for the child processes where it will just supply the required CloneFlags to setup a new PID, Mount and Network namepsace. 

See [Tradeoffs](#tradeoffs) section for information about how this should be improved

## Client

The client CLI will have a top level command for each feature it supports:

```
Usage: cu-thor <command> [<args>]

Options:
  --help, -h             display this help and exit

Commands:
  start                  starts the remote execution of a command on the server
  list                   shows information about the currently available executions
  stop                   stops an execution
  status                 returns the information about a certain execution
  output                 streams the output of a certain execution
```

The `start` command will send a command to be started to the server, along with the arguments passed to it

```
Usage: cu-thor start [--command COMMAND] [ARGS [ARGS ...]]

Positional arguments:
  ARGS                   these arguments will be supplied to the executed command

Options:
  --command COMMAND      the command to be executed on the server

```

The example of `cu-thor start --command head /dev/random` will result in executing the command `head` with the argument `/dev/random`
Similarly the command `cu-thor start --command head -- -n10 /dev/random` will execute the command `head` with the arguments `-n10` and `/dev/random`

The `list` subcommand will return the current executions that this client has rights to interact with

The `status`, `stop`, `output`, subcommands all require the `--id` parameter must be supplied with the id of the execution

The client credentials for simplicty's sake will be read from `~/.cu-thor/creds/` but could be overwritten by the `CUTHOR_CLIENT_CRED_LOCATION` environment variable 

# Tradeoffs

## Log storage

The in memory log storage has two serious drawbacks. 
* A single offending process can fill the memory of the server with output
* The executions metadata needs to be kept in memory after the execution have finished, a recycling of the executions should be implemented

One mitigation step would be to store the output of the single processes in a predefined directory. This would allow the server to rely on this directory to return the output. The downside of this is that the app needs to implement a `tail -f` type function to stream the data the client

## Cert generation

The current cert generation with the script is not a good way to easily provision the required certs to new client. Also the company will probably wish to use it's own CA to sign an intermediate CA that will sign the CSR for the server and the clients. This should be automated depending on the setup of how the clients actually use the application

## Persistent storage

Execution metadata and output live only as far as the server keeps it in memory. After a server crash/restart all previous executions are lost. This could be acceptable, but most likely some form of persistent storage should be introduced

## Resource limits

The app will start the process and then add its PID to the appropriate file in the corresponding cgroup. This is a working solution, but it would be nicer to start an auxiliary app that will add itself the correct cgroup then start up the passed process as its child automatically in the cgroup. There is such a command in the `libcgroup-tools` package called `cgexec`

Since the app limits each process to a certain % of CPU time we can over allocate the available CPU time the server should either limit the number of allowed parallel processes, or should encapsulate the cgroups in an outer cgroup that for safety reasons sets a limit on the cumulative resource usage of the all the started processes

## Isolation

The app will start the underlying processes in their own linux namespaces by passing the appropriate clone flags to the command executor. This will however not provide full isolation. In order to create a fully isolated environment the following steps should be taken:
* Setup an own `/proc` directory to limit the visiblity of the host processes and the host mounts
* Setup a veth network interface to be able to communicate with the outside world using the hosts network interfaces
